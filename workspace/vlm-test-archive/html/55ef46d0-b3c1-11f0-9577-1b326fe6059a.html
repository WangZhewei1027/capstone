<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>K-Nearest Neighbors (KNN)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
        }
        h1 {
            color: #333;
        }
        p {
            line-height: 1.6;
        }
        .container {
            max-width: 600px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>K-Nearest Neighbors (KNN)</h1>
        <p>The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful, supervised machine learning algorithm used for both classification and regression tasks. The idea behind KNN is to find the 'k' number of closest training examples in the feature space to a given query point, and then make predictions based on the labels or values of those neighbors.</p>
        <h2>How It Works:</h2>
        <ol>
            <li>Choose the number of neighbors 'k'.</li>
            <li>Calculate the distance between the query point and all training points.</li>
            <li>Sort the distances and determine the nearest 'k' points.</li>
            <li>For classification, take a majority vote among the neighbors. For regression, compute the average of the neighbors' values.</li>
        </ol>
        <h2>When to Use KNN:</h2>
        <p>KNN is useful when you have a small dataset and want to make quick predictions. However, it can become inefficient with large datasets, as it requires distance calculations for every query.</p>
    </div>
</body>
</html>